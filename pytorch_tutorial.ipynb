{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial\n",
    "[Pytorch Tutorial学习笔记](https://www.yuque.com/hansonyun/cy91ib/ukvxkoi1s5ab6r5u) 这个是自己的学习教程，主要是为了记录自己的学习过程，方便以后查阅，也希望能够帮助到其他人。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tensor操作\n",
    "记录Tensor的一些操作示例，实践了才知道怎么使用。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor类型查看及检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Create a tensor of size (2, 3) from a normal distribution\n",
    "a = torch.randn(2, 3)\n",
    "\n",
    "b = torch.FloatTensor(2, 3)\n",
    "# Show tensor a type\n",
    "print(a.type())\n",
    "print(type(a)) # 这块有个疑问，为什么type(a)和a.type()的结果不一样？ 理论上来说type()也是输出子类的情况啊\n",
    "\n",
    "# Check tensor type\n",
    "print(isinstance(a, torch.Tensor))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor 数据从CPU转到GPU\n",
    "Tensor数据在CPU和GPU上时是不同的类型，在不同的设备上进行计算时，需要将数据转到相同的设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Create a tensor of size (2, 3) from a normal distribution\n",
    "a = torch.randn(2,3)\n",
    "\n",
    "# print current tensor device\n",
    "print(a.device)\n",
    "\n",
    "# transfer tensor to GPU 0\n",
    "if torch.cuda.is_available():\n",
    "    a = a.cuda()\n",
    "    print(a.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同维度的Tensor及其属性\n",
    "Tensor的维度是指Tensor的阶数，也就是Tensor的维度数目，比如一维Tensor的维度是1，二维Tensor的维度是2，以此类推。Tensor的维度属性是一个元组，元组的长度就是Tensor的维度，元组中的每个元素代表了Tensor在该维度上的长度。其维度可以通过Tensor的dim()方法查看，也可以通过Tensor的ndimension()方法查看。\n",
    "查看其属性可以通过Tensor的shape属性，也可以通过Tensor的size()方法。查看元素的个数可以通过Tensor的numel()方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 0维Tensor，标量，可以用于表示loss\n",
    "a = torch.tensor(1.0)\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "# 1维Tensor，向量，可以用于表示bias\n",
    "a = torch.tensor([1.0])\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "a = torch.tensor([1.0,2.0,3.0,4.0])\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "a = torch.FloatTensor(2)\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "# 2维Tensor，矩阵，可以用于表示weight\n",
    "a = torch.tensor([[1.0,2.0],[3.0,4.0]])\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "a = torch.FloatTensor(2,3)\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "# 3维Tensor，可以用于表示RNN的输入\n",
    "a = torch.tensor([[[1.0,2.0],[3.0,4.0]],[[5.0,6.0],[7.0,8.0]]])\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "a = torch.FloatTensor(2,3,4)\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "\n",
    "# 4维Tensor，可以用于表示CNN的输入\n",
    "a = torch.tensor([[[[1.0,2.0],[3.0,4.0]],[[5.0,6.0],[7.0,8.0]]],[[[9.0,10.0],[11.0,12.0]],[[13.0,14.0],[15.0,16.0]]]])\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n",
    "a = torch.FloatTensor(2,3,4,5)\n",
    "print('dim',a.dim(),a.ndimension())\n",
    "print('shape',a.shape)\n",
    "print('size',a.size())\n",
    "print('numel',a.numel())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor的基本操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的创建\n",
    "- 固定元素初始化创建(指定每个元素的值): 从list创建, torch.tensor();从numpy创建，torch.from_numpy()\n",
    "- 固定元素初始化创建(指定Tensor shape): torch.eye(), torch.zeros(), torch.zeros_like(), torch.ones(), torch.ones_like(), torch.full(), torch.full_like(), torch.range(), torch.arange(), torch.linspace(), torch.logspace()\n",
    "- 随机元素初始化创建(值随机选取): torch.empty(), torch.empty_like(), torch.Tensor(), torch.FloatTensor(), torch.IntTensor(), torch.DoubleTensor()\n",
    "- 随机元素初始化创建(指定值分布): torch.rand()，torch.rand_like(), torch.randint(), torch.randn()，torch.randn_like(), torch.normal(), torch.randperm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ------------------------------固定元素初始化创建(指定每个元素的值)--------------------------------\n",
    "# 从list创建Tensor\n",
    "a = torch.tensor(1.0, dtype=torch.float32); print(a)\n",
    "a = torch.tensor([1.0,2.0,3.0,4.0]); print(a)\n",
    "a = torch.tensor([[1.0,2.0],[3.0,4.0]]); print(a)\n",
    "\n",
    "# 从numpy创建Tensor\n",
    "a = torch.from_numpy(np.array([1.0,2.0,3.0,4.0])); print(a)\n",
    "\n",
    "#-----------------------------固定元素初始化创建(指定Tensor shape)--------------------------------\n",
    "# 从eye创建Tensor,只能创建2D Tensor\n",
    "a = torch.eye(3,4); print('eye', a)\n",
    "\n",
    "# 从zeros, zeros_like创建Tensor\n",
    "a = torch.zeros(3,4); print('zeros', a)\n",
    "a = torch.zeros_like(a); print('zeros_like', a)\n",
    "\n",
    "# 从ones, ones_like创建Tensor\n",
    "a = torch.ones(3,4); print('ones', a)\n",
    "a = torch.ones_like(a); print('ones_like', a)\n",
    "\n",
    "\n",
    "# 从full，full_like创建Tensor\n",
    "a = torch.full((3,4), 1.0); print('full', a)\n",
    "a = torch.full_like(a, 2.0); print('full_like', a)\n",
    "\n",
    "# 从range创建Tensor， range左闭右闭，且必须输入两个值; torch.range即将被弃用\n",
    "a = torch.range(1, 10); print('range', a)\n",
    "\n",
    "# 从arange创建Tensor，arange左闭右开\n",
    "a = torch.arange(1, 10, 2); print('arange', a)\n",
    "a = torch.arange(10); print('arange', a) # 从0开始, 默认步长为1\n",
    "a = torch.arange(10,0,-1); print('arange', a) # 从10开始，步长为-1\n",
    "\n",
    "# 从linespace创建Tensor，linespace左闭右闭\n",
    "a = torch.linspace(1, 10, 20); print('linspace', a)\n",
    "\n",
    "# 从logspace创建Tensor，logspace左闭右闭\n",
    "a = torch.logspace(1, 10, 20, 2); print('logspace', a) # 从[1,10]生成20个数，然后以2为底进行指数运算\n",
    "\n",
    "# ------------------------------------随机元素初始化创建(值随机选取)--------------------------------\n",
    "# 从empty创建Tensor\n",
    "a = torch.empty(3,4); print('empty', a)\n",
    "a = torch.empty_like(a); print('empty_like', a)\n",
    "\n",
    "# 从Tensor创建Tensor\n",
    "a = torch.Tensor(3,4); print('Tensor', a)\n",
    "a = torch.FloatTensor(3,4); print('FloatTensor', a)\n",
    "a = torch.IntTensor(3,4); print('IntTensor', a)\n",
    "a = torch.LongTensor(3,4); print('LongTensor', a)\n",
    "\n",
    "# ------------------------------------随机元素初始化创建(指定值分布)-------------------------------- \n",
    "# 从rand创建Tensor, rand为均匀分布\n",
    "a = torch.rand(3,4); print('rand', a)\n",
    "a = torch.rand_like(a); print('rand_like', a)\n",
    "\n",
    "# 从randint创建Tensor, randint为均匀分布\n",
    "a = torch.randint(0, 10, (3,4)); print('randint', a)\n",
    "\n",
    "# 从randn创建Tensor, randn为标准正态分布\n",
    "a = torch.randn(3,4); print('randn', a)\n",
    "a = torch.randn_like(a); print('randn_like', a)\n",
    "\n",
    "# 从normal创建Tensor, normal为正态分布\n",
    "a = torch.normal(mean=torch.full((10,), 0.0), std=torch.arange(1.0, 0.0, -0.1)); print('normal', a)\n",
    "\n",
    "# 从randperm创建Tensor, randperm为随机排列，用于生成随机不重复整数序列\n",
    "a = torch.randperm(10); print('randperm', a)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor索引及切片\n",
    "Tensor的索引及切片操作与numpy的操作类似，但是Tensor的索引及切片操作是在原Tensor上进行的，而numpy的索引及切片操作是在原数据的拷贝上进行的。也就是说，索引得到的是原数据的引用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4,3,28,28)\n",
    "print(\"a\",a)\n",
    "\n",
    "# 验证索引是原数据的引用\n",
    "a[0][0][0][0] = 2023.0\n",
    "print(\"change a\", a)\n",
    "\n",
    "# 索引单个位置\n",
    "a_0 = a[0]; print('a_0', a_0.shape)\n",
    "a_0_0 = a[0][0]; print('a_0_0', a_0_0.shape) # 或a[0,0]\n",
    "a_0_0_0 = a[0][0][0]; print('a_0_0_0', a_0_0_0.shape) # 或a[0,0,0]\n",
    "a_0_0_0_0 = a[0][0][0][0]; print('a_0_0_0_0', a_0_0_0_0, a_0_0_0_0.shape) # 或a[0,0,0,0]\n",
    "\n",
    "# 索引前N个或后N个\n",
    "a_2 = a[:2]; print('a_2', a_2.shape)\n",
    "a_2_1 = a[:2,:1]; print('a_2_1', a_2_1.shape)\n",
    "a_2_1_3 = a[:2,:1,:3]; print('a_2_1_3', a_2_1_3.shape) # 或a[:2,:1,:3]\n",
    "\n",
    "# 根据步长索引\n",
    "a_s_2 = a[::2]; print('a_s_2', a_s_2.shape)\n",
    "a_s_2 = a[:,::2,1:10:2,:]; print('a_s_2', a_s_2.shape)\n",
    "\n",
    "# 根据index_select索引,维度，索引具体位置\n",
    "a_0 = a.index_select(0, torch.tensor([0,1])); print('a_0', a_0.shape)\n",
    "\n",
    "# 使用...索引，...表示任意多维, 会根据前后维度自动补全\n",
    "a_0 = a[0,...]; print('a_0', a_0.shape)\n",
    "a_0 = a[0,...,1]; print('a_0', a_0.shape)\n",
    "a_0 = a[0,...,1:10:2]; print('a_0', a_0.shape)\n",
    "\n",
    "# 使用masked_select索引, 默认会将Tensor转换为1D Tensor\n",
    "mask = a.ge(0.5); print('mask', mask)\n",
    "a_0 = a.masked_select(mask); print('a_0', a_0.shape)\n",
    "\n",
    "# 使用take索引, 默认会将Tensor转换为1D Tensor, 然后在根据传入的1D Tensor进行索引\n",
    "a_0 = a.take(torch.tensor([0,1,2,3,4,5,6,7,8,9])); print('a_0', a_0.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的维度变换\n",
    "- view/reshape: view和reshape的作用是将Tensor的维度变换为指定的维度，但是view只能用于contiguous Tensor，而reshape可以用于任意Tensor。这里需要注意view和reshape后的内容要有意义。\n",
    "- squeeze/unsqueeze: squeeze和unsqueeze的作用是一样的，都是在Tensor的指定维度上增加或者减少一个维度。unsqueeze有个口诀，传入正数在之前插入，传入负数在之后插入。\n",
    "- transpose/t/permute: transpose和t的作用是一样的，都是将Tensor的两个维度进行交换，permute可以对Tensor的多个维度进行交换。\n",
    "- expand/repeat: expand和repeat的作用是一样的，都是将Tensor的指定维度进行复制; expand要求Tensor在指定维度上的长度为1，而repeat没有这个要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view操作进行Tensor维度变换, 更改前后元素数量要一致\n",
    "a = torch.randn(4,3,28,28)\n",
    "a_0 = a.view(4,3,28*28); print('a_0', a_0.shape) # 合并最后两个维度，从图像上来理解的话，变成了4张图像，每张图像有3个通道，每个通道有784个像素点\n",
    "a_0 = a.view(4,3*28,28); print('a_0', a_0.shape) # 合并中间两个维度，从图像上来理解的话，变成了4张图像，每张图像有84行，每行有28个像素点\n",
    "a_0 = a.view(4*3,28,28); print('a_0', a_0.shape) # 合并前两个维度，从图像上来理解的话，变成了12张图像，每张图像有28行，每行有28个像素点\n",
    "\n",
    "# reshape操作进行Tensor维度变换, 更改前后元素数量要一致\n",
    "a_1 = a.reshape(4,3,28*28); print('a_1', a_1.shape) # 合并最后两个维度，从图像上来理解的话，变成了4张图像，每张图像有3个通道，每个通道有784个像素点\n",
    "a_1 = a.reshape(4,3*28,28); print('a_1', a_1.shape) # 合并中间两个维度，从图像上来理解的话，变成了4张图像，每张图像有84行，每行有28个像素点\n",
    "a_1 = a.reshape(4*3,28,28); print('a_1', a_1.shape) # 合并前两个维度，从图像上来理解的话，变成了12张图像，每张图像有28行，每行有28个像素点\n",
    "\n",
    "# squeeze操作进行Tensor维度变换, 去除维度为1的维度\n",
    "a = torch.randn(1,3,1,1)\n",
    "a_2 = a.squeeze(); print('a_2', a_2.shape)\n",
    "a_2 = a.squeeze(0); print('a_2', a_2.shape)\n",
    "a_2 = a.squeeze(2); print('a_2', a_2.shape)\n",
    "\n",
    "# unsqueeze操作进行Tensor维度变换, 在指定位置增加维度为1的维度\n",
    "a = torch.randn(3,28,28)\n",
    "a_3 = a.unsqueeze(0); print('a_3', a_3.shape)\n",
    "a_3 = a.unsqueeze(1); print('a_3', a_3.shape)\n",
    "a_3 = a.unsqueeze(-1); print('a_3', a_3.shape)\n",
    "\n",
    "# t操作进行Tensor维度变换, 交换维度; 只能用于2D Tensor\n",
    "a = torch.randn(3,4)\n",
    "a_4 = a.t(); print('a_4', a_4.shape)\n",
    "\n",
    "# transpose操作进行Tensor维度变换, 交换维度; 可以用于多维Tensor\n",
    "a = torch.randn(3,4,5)\n",
    "a_5 = a.transpose(1,2); print('a_5', a_5.shape)\n",
    "a_5 = a.transpose(0,2); print('a_5', a_5.shape)\n",
    "a_5 = a.transpose(0,1); print('a_5', a_5.shape)\n",
    "\n",
    "# permute操作进行Tensor维度变换, 交换维度; 可以用于多维Tensor\n",
    "a = torch.randn(3,4,5)\n",
    "a_6 = a.permute(1,2,0); print('a_6', a_6.shape)\n",
    "a_6 = a.permute(2,0,1); print('a_6', a_6.shape)\n",
    "a_6 = a.permute(0,2,1); print('a_6', a_6.shape)\n",
    "\n",
    "# expand操作进行Tensor维度变换, 如果待扩展维度为1，则扩展为指定维度；如果待扩展维度不为1且与原维度不一致，则报错\n",
    "a = torch.randn(1,4,5)\n",
    "a_7 = a.expand(3,4,5); print('a_7', a_7.shape)\n",
    "\n",
    "# repeat操作进行Tensor维度变换, 传入参数为每个维度的重复次数\n",
    "a = torch.randn(3,4,5)\n",
    "a_8 = a.repeat(2,1,1); print('a_8', a_8.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Broadcast自动扩展\n",
    "tensor broadcast是指在进行运算时，如果两个tensor的shape不一样，会自动扩展成一样的shape，然后再进行运算。tensor broadcast的规则如下：\n",
    "- 对两个tensor的shape从后往前进行比较，如果两个tensor的shape在某个维度上相同或者其中一个tensor在该维度上的长度为1，则这两个tensor在该维度上是兼容的，可以进行broadcast。\n",
    "- 如果两个tensor在某个维度上都不为1，且两个tensor在该维度上的长度不相等，则这两个tensor在该维度上是不兼容的，不能进行broadcast。\n",
    "- 满足第一个条件的情况下，如果两个tensor的维度数目不同，则在较小的tensor的shape前面补1，使得两个tensor的维度数目相同，然后再进行比较。\n",
    "  \n",
    "为什么要使用Broadcast？ 一是有实际需求，二是可以减少内存的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast操作进行Tensor自动扩展\n",
    "a = torch.randn(3,4,5)\n",
    "b = torch.randn(4,5)\n",
    "c = a + b; print('c', c.shape)\n",
    "\n",
    "a = torch.randn(3,1)\n",
    "b = torch.randn(1,3)\n",
    "c = a + b; print('c', c.shape)\n",
    "\n",
    "a = torch.randn(3,1,5)\n",
    "b = torch.tensor([0.5])\n",
    "c = a * b; print('c', c.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor拼接及分割\n",
    "- cat: cat是concatenate的缩写，指的是将多个Tensor在指定维度上进行拼接;要求除了指定维度外，其他维度的长度必须一致。\n",
    "- stack: stack是将多个Tensor在指定维度上进行堆叠，堆叠后的Tensor的维度比原来的Tensor的维度多1;要求待堆叠的Tensor的shape必须一致。\n",
    "- split: split是将Tensor在指定维度上进行分割\n",
    "- chunk: chunk是将Tensor在指定维度上进行分割；chunk是分块的意思，从官网上的意思看，如果可以整除，则每个块的长度相同，如果不能整除，则最后一个块的长度可能会不同（是不是这里隐含着最后一个块的大小要小于其他块）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat操作进行Tensor拼接, 拼接的维度必须一致\n",
    "a = torch.randn(3,4)\n",
    "b = torch.randn(3,5)\n",
    "c = torch.cat([a,b], dim=1); print('c', c.shape)\n",
    "\n",
    "# stack操作进行Tensor拼接, 拼接的维度必须一致；dim指定拼接的维度\n",
    "a = torch.randn(3,4)\n",
    "b = torch.randn(3,4)\n",
    "c = torch.stack([a,b], dim=0); print('c', c.shape)\n",
    "\n",
    "# split操作进行Tensor拆分\n",
    "a = torch.randn(3,4)\n",
    "b, c = a.split([1,2], dim=0); print('b', b.shape,'c', c.shape)\n",
    "b, c, d = a.split([1,1,2], dim=1); print('b', b.shape,'c', c.shape, 'd', d.shape)\n",
    "\n",
    "# chunk操作进行Tensor拆分\n",
    "a = torch.randn(3,5)\n",
    "b, c = a.chunk(2, dim=0); print('b', b.shape,'c', c.shape)\n",
    "b, c, d = a.chunk(3, dim=1); print('b', b.shape,'c', c.shape, 'd', d.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的基本运算\n",
    "- Tensor逐元素加减乘除: +, -, *, /, add, sub, mul, div\n",
    "- Tensor乘法： @, matmul, mm(只适合二维Tensor)\n",
    "- Tensor逐元素n次方: pow(**), sqrt, rsqrt\n",
    "- Tensor逐元素取对数: log, log2, log10, log1p\n",
    "- Tensor逐元素取指数: exp, exp2\n",
    "- Tensor逐元素取近似值: ceil, floor, round, trunc, frac\n",
    "- Tensor逐元素取余: fmod, remainder；两种只是在trunc_mode上有些不同\n",
    "- Tensor逐元素取绝对值: abs, absolute\n",
    "- Tensor逐元素取符号: sign\n",
    "- Tensor逐元素取上下限: clamp, clamp_max, clamp_min, clip\n",
    "- Tensor逐元素取反: neg\n",
    "- Tensor逐元素比较: >, >=, <, <=, ==, !=, gt, ge, lt, le, eq, ne，isclose\n",
    "- Tensor之间比较: allclose, equal\n",
    "- Tensor逐元素逻辑运算: &, |, ^, ~, logical_and, logical_or, logical_xor, logical_not\n",
    "- Tensor逐元素取倒数: reciprocal\n",
    "- Tensor逐元素取三角函数: sin, cos, tan, asin, acos, atan, atan2, sinh, cosh, tanh, asinh, acosh, atanh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Tensor逐元素加减乘除，要求两个Tensor的shape一致\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(3,4); print('b', b)\n",
    "c = a + b; print('+', c)\n",
    "c = a.add(b); print('add', c)\n",
    "c = a - b; print('-', c)\n",
    "c = a.sub(b); print('sub', c)\n",
    "c = a * b; print('*', c)\n",
    "c = a.mul(b); print('mul', c)\n",
    "c = a / b; print('/', c)\n",
    "c = a.div(b); print('div', c)\n",
    "\n",
    "# Tensor 二维mm乘法，要求符合矩阵乘法规则\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(4,5); print('b', b)\n",
    "c = torch.mm(a,b); print('mm', c)\n",
    "\n",
    "# Tensor matmul乘法，要求符合矩阵乘法规则, 除最后两维外，其他维度必须一致\n",
    "a = torch.randn(3,4,5); print('a', a)\n",
    "b = torch.randn(3,5,6); print('b', b)\n",
    "c = torch.matmul(a,b); print('matmul', c)\n",
    "c = a@b; print('@', c)\n",
    "\n",
    "# Tensor 逐元素n次方\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.pow(2); print('pow', c)\n",
    "c = a**2; print('**', c)\n",
    "c = a.sqrt(); print('sqrt', c)      # sqrt 为平方根\n",
    "c = a.rsqrt(); print('rsqrt', c)    # rsqrt 为平方根倒数(reciprocal of the square root)\n",
    "\n",
    "# Tensor 逐元素取对数\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.log(); print('log', c)\n",
    "c = a.log2(); print('log2', c)\n",
    "c = a.log10(); print('log10', c)\n",
    "c = a.log1p(); print('log1p', c)    # log1p 为log(1+x)\n",
    "\n",
    "# Tensor 逐元素取指数\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.exp(); print('exp', c)\n",
    "c = a.exp2(); print('exp2', c)    # exp2 为2的指数\n",
    "\n",
    "# Tensor 逐元素近似\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.ceil(); print('ceil', c)    # ceil 为向上取整\n",
    "c = a.floor(); print('floor', c)  # floor 为向下取整\n",
    "c = a.round(); print('round', c)  # round 为四舍五入\n",
    "c = a.trunc(); print('trunc', c)  # trunc 为截断取整\n",
    "c = a.frac(); print('frac', c)    # frac 为取小数部分\n",
    "\n",
    "# Tensor 逐元素取余\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.remainder(1); print('remainder', c)    # remainder: torch.remainder(a, b) == a - a.div(b, rounding_mode=\"floor\") * b\n",
    "c = a.fmod(1); print('fmod', c)              # fmod: torch.fmod(a, b) == a - a.div(b, rounding_mode=\"trunc\") * b\n",
    "\n",
    "# Tensor 逐元素取绝对值\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.abs(); print('abs', c)\n",
    "c = a.absolute(); print('absolute', c)\n",
    "\n",
    "# Tensor 逐元素取符号\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.sign(); print('sign', c)\n",
    "\n",
    "# Tensor 逐元素取上下限\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.clamp(-0.1, 0.1); print('clamp', c)\n",
    "c = a.clamp_max(0.1); print('clamp_max', c)\n",
    "c = a.clamp_min(-0.1); print('clamp_min', c)\n",
    "c = a.clip(-0.1, 0.1); print('clip', c)\n",
    "\n",
    "# Tensor 逐元素取反\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.neg(); print('neg', c)\n",
    "\n",
    "# Tensor 逐元素比较\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(3,4); print('b', b)\n",
    "c = a > b; print('>', c)\n",
    "c = a >= b; print('>=', c)\n",
    "c = a < b; print('<', c)\n",
    "c = a <= b; print('<=', c)\n",
    "c = a == b; print('==', c)\n",
    "c = a != b; print('!=', c)\n",
    "c = a.isclose(b); print('isclose', c)\n",
    "\n",
    "# Tensor 比较\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(3,4); print('b', b)\n",
    "c = torch.equal(a, b); print('equal', c)\n",
    "c = torch.allclose(a, b); print('allclose', c)\n",
    "\n",
    "# Tensor 逐元素逻辑运算\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(3,4); print('b', b)\n",
    "c = torch.logical_and(a>0, b>0); print('logical_and', c)\n",
    "c = torch.logical_or(a>0, b>0); print('logical_or', c)\n",
    "c = torch.logical_xor(a>0, b>0); print('logical_xor', c)\n",
    "c = torch.logical_not(a>0); print('logical_not', c)\n",
    "\n",
    "# Tensor 逐元素取倒数\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.reciprocal(); print('reciprocal', c)\n",
    "\n",
    "# Tensor 逐元素取三角函数\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.sin(); print('sin', c)\n",
    "c = a.cos(); print('cos', c)\n",
    "c = a.tan(); print('tan', c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的属性及统计信息\n",
    "- Tensor的最大最小值及位置: max, min，argmax, argmin\n",
    "- Tensor的均值及方差: mean, std\n",
    "- Tensor指定维度 的累加和累乘: sum, prod; \n",
    "- Tensor指定维度的累加和累乘: cumsum, cumprod; 会保持原Tensor的维度，只是在指定维度上进行累加或累乘\n",
    "- Tensor的范数: norm\n",
    "- Tensor的距离: dist\n",
    "- Tensor的非零元素: nonzero\n",
    "- Tensor的k统计属性: topk, kthvalue\n",
    "- Tensor的唯一元素: unique; 返回剔除重复元素后的Tensor\n",
    "- Tensor的元素属性判断: isfinite, isinf, isnan, isneginf, isposinf\n",
    "- Tensor的统计: bincount, count_nonzero, histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Tensor的最大值最小值及其索引，可以使用dim参数指定维度以及keepdim参数保持维度不变\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.max(dim=1,keepdim=True); print('max', c)\n",
    "c = a.min(); print('min', c)\n",
    "c = a.argmax(dim=1,keepdim=True); print('argmax', c)\n",
    "c = a.argmin(); print('argmin', c)\n",
    "\n",
    "# Tensor的均值及方差, 可以使用dim参数指定维度以及keepdim参数保持维度不变\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.mean(dim=1,keepdim=True); print('mean', c)\n",
    "c = a.std(); print('std', c)\n",
    "\n",
    "# Tensor所有要素的累加和累积\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.sum(); print('sum', c)\n",
    "c = a.prod(); print('prod', c)\n",
    "\n",
    "# Tensor所有要素的累加和累积，保持维度不变\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.cumsum(dim=1); print('cumsum', c)\n",
    "c = a.cumprod(dim=1); print('cumprod', c)\n",
    "\n",
    "# Tensor的范数，范数的计算公式为：norm = (x1^p + x2^p + ... + xn^p)^(1/p)\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.norm(1); print('norm-1', c)\n",
    "c = a.norm(2); print('norm-2', c)\n",
    "\n",
    "# Tensor的距离，距离的计算公式为：dist = (x1-y1)^p + (x2-y2)^p + ... + (xn-yn)^p)^(1/p)\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(3,4); print('b', b)\n",
    "c = torch.dist(a, b, 1); print('dist-1', c)\n",
    "c = torch.dist(a, b, 2); print('dist-2', c)\n",
    "\n",
    "# Tensor的非0元素, 返回的是非0元素的索引\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.nonzero(); print('nonzero', c)\n",
    "\n",
    "# Tensor的k属性统计\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.kthvalue(2,dim=0); print('kthvalue', c)\n",
    "c = a.topk(2,dim=0); print('topk', c)\n",
    "\n",
    "# Tensor的unique属性统计\n",
    "a = torch.tensor([1,2,3,4,5,6,7,8,9,10,1,2,3,4,5]); print('a', a)\n",
    "c = a.unique(); print('unique', c)\n",
    "\n",
    "# Tensor逐元素判断特性\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = a.isfinite(); print('isfinite', c)\n",
    "c = a.isinf(); print('isinf', c)\n",
    "c = a.isnan(); print('isnan', c)\n",
    "c = a.isposinf(); print('isposinf', c) # 正无穷\n",
    "c = a.isneginf(); print('isneginf', c) # 负无穷\n",
    "\n",
    "# Tensor的统计信息\n",
    "a = torch.randint(1,100,(10,),dtype=torch.int32); print('a', a)\n",
    "c = a.bincount(); print('bincount', c) # torch bincount只能处理非负整数\n",
    "c = a.count_nonzero(); print('count_nonzero', c) # 输出非0元素的个数\n",
    "a = torch.randn(3,4,5); print('a', a)\n",
    "c = a.histogram(bins=4); print('histogram', c) # 输出一维直方图,就是将数据分成bins个区间，然后统计每个区间的个数\n",
    "\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "c = torch.histogramdd(a, bins=[3,5,3,3]); print('histogramdd', c) # 输出多维直方图,这里所谓的多维直方图就是每一列的直方图;就是将数据分成bins个区间，然后统计每个区间的个数\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的一些高级操作\n",
    "- Tensor的where操作: where指的是根据条件从两个Tensor中选择元素组成一个新的Tensor, 使用where的好处是可以并行化处理。\n",
    "- Tensor的gather操作: gather的作用是根据索引从Tensor中取出元素组成一个新的Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor的where操作\n",
    "a = torch.randn(3,4); print('a', a)\n",
    "b = torch.randn(3,4); print('b', b)\n",
    "c = torch.where(a>0, a, b); print('where', c)\n",
    "\n",
    "# Tensor的gather操作\n",
    "prob = torch.randn(4,10); print('prob', prob)\n",
    "label = torch.arange(10)*100; print('label', label)\n",
    "idx = prob.topk(dim=1, k=3); print('idx', idx)\n",
    "c = torch.gather(label.expand(4,10), dim=1, index=idx.indices); print('gather', c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降\n",
    "pytorch 在计算梯度的时候，实际上是把计算图保存在了输出的Tensor里面了，比如通过mse_loss或者直接通过手写公式计算的时候，都会把计算图保存在输出的Tensor里面，然后通过backward函数来计算梯度，这样就可以实现自动求导了。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数及其梯度及其梯度计算\n",
    "- sigmoid激活函数: 其表达式为$\\sigma(x) = \\frac{1}{1+e^{-x}}$, 其梯度为$\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$。其缺点是当x的绝对值很大时，其梯度会趋近于0，导致梯度消失。\n",
    "- tanh激活函数: 其表达式为$\\tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$, 其梯度为$\\tanh'(x) = 1-\\tanh^2(x)$。其缺点是当x的绝对值很大时，其梯度会趋近于0，导致梯度消失。\n",
    "- ReLU激活函数: 其表达式为$\\text{ReLU}(x) = \\max(0, x)$, 其梯度为$\\text{ReLU}'(x) = \\begin{cases} 0, & x<0 \\\\ 1, & x>0 \\end{cases}$。其缺点是当x<0时，其梯度为0，导致梯度消失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Tensor sigmoid 激活函数\n",
    "a = torch.randn(10); print('a', a)\n",
    "c = torch.sigmoid(a); print('sigmoid', c)\n",
    "\n",
    "# Tensor tanh 激活函数\n",
    "a = torch.randn(10); print('a', a)\n",
    "c = torch.tanh(a); print('tanh', c)\n",
    "\n",
    "# Tensor relu 激活函数\n",
    "a = torch.randn(10); print('a', a)\n",
    "c = torch.relu(a); print('relu', c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss函数及其梯度计算\n",
    "- MSE(Mean Square Error)均方误差: 其表达式为$\\text{loss}(y, f_{\\theta}(x)) = \\sum_{i=1}^n(y_i-f_{\\theta}(x_i))^2$，其梯度为$\\frac{\\partial \\text{loss}(y, f_{\\theta}(x))}{\\partial \\theta} =\\sum_{i=1}^n 2(y_i-f_{\\theta}(x_i)) * \\frac{\\partial{f_{\\theta}(x_i)}}{\\partial{\\theta}}$。\n",
    "- SoftMax函数: Cross Entropy Loss一般搭配SoftMax函数使用。SoftMax函数的表达式为$\\text{SoftMax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$，其梯度为$\\frac{\\partial \\text{SoftMax}(x_i)}{\\partial x_j} = \\begin{cases} \\text{SoftMax}(x_i)(1-\\text{SoftMax}(x_i)) & i == j \\\\ -\\text{SoftMax}(x_j)*\\text{SoftMax}(x_i) & i \\neq j \\end{cases}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor MSE MSE损失函数\n",
    "x = torch.full((1,), 1); print('x', x)\n",
    "w = torch.full((1,), 2.0); print('w', w)\n",
    "mse = torch.nn.functional.mse_loss(torch.ones(1), x*w); print('mse', mse)\n",
    "\n",
    "## 错误示例1: w没有设置requires_grad=True，导致无法求导\n",
    "# torch.autograd.grad(mse, w)\n",
    "\n",
    "## 错误示例2: w虽然设置了requires_grad=True，但是没有更新计算图，导致无法求导\n",
    "# w.requires_grad_(True)\n",
    "# torch.autograd.grad(mse, w)\n",
    "\n",
    "# 正确示例: w设置了requires_grad=True，并且更新了计算图，导致可以求导\n",
    "w.requires_grad_(True)\n",
    "mse = torch.nn.functional.mse_loss(torch.ones(1), x*w); print('mse', mse)\n",
    "torch.autograd.grad(mse, w)\n",
    "\n",
    "# 正确实例，使用backward函数求导\n",
    "w.requires_grad_(True)\n",
    "mse = torch.nn.functional.mse_loss(torch.ones(1), x*w); print('mse', mse)\n",
    "mse.backward();print('w.grad', w.grad)\n",
    "\n",
    "\n",
    "# Tensor SoftMax 激活函数\n",
    "a = torch.randn(10,requires_grad=True); print('a', a)\n",
    "c = torch.softmax(a, dim=0); print('softmax', c)\n",
    "p = torch.nn.functional.softmax(a, dim=0); print('softmax', p)\n",
    "\n",
    "## 错误示例1: 对于loss函数求导，其output只能是一个标量，而不能是一个向量\n",
    "# p.backward();print('a.grad', a.grad)\n",
    "#\n",
    "# 正确示例，使用grad求导\n",
    "print(torch.autograd.grad(p[0], a,retain_graph=True)) # retain_graph=True表示保留计算图，否则会被释放\n",
    "print(torch.autograd.grad(p[1], a))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单层感知机反向传播参数更新\n",
    "- 单层单输出感知机梯度计算\n",
    "- 单层多输出感知机梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 单层单输出感知机\n",
    "x = torch.randn(1,10); print('x', x)\n",
    "w = torch.randn(1,10,requires_grad=True); print('w', w)\n",
    "o = torch.sigmoid(x@w.t()); print('o', o)\n",
    "mse = torch.nn.functional.mse_loss(torch.ones(1,1), o)\n",
    "mse.backward();print('w.grad', w.grad)\n",
    "\n",
    "# 单层多输出感知机\n",
    "x = torch.randn(1,10); print('x', x)\n",
    "w = torch.randn(3,10,requires_grad=True); print('w', w)\n",
    "o = torch.sigmoid(x@w.t()); print('o', o)\n",
    "mse = torch.nn.functional.mse_loss(torch.ones(1,3), o); print('mse', mse)\n",
    "mse.backward();print('w.grad', w.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d函数优化实例，主要使用梯度下降法\n",
    "Himmelblau函数的表达式为$f(x, y) = (x^2+y-11)^2+(x+y^2-7)^2$，其最小值所在的位置为（3，2），（-2.805，3.131），（-3.7793，-3.283），（3.58，-1.84）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示梯度下降算法\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def himmelblau(x):\n",
    "    return (x[0]**2+x[1]-11)**2+(x[0]+x[1]**2-7)**2\n",
    "\n",
    "def plot_himmelblau(x,y):\n",
    "                        \n",
    "    X, Y = np.meshgrid(x, y) # meshgrid函数用两个坐标轴上的点在平面上画格, 输出的是两个二维矩阵，第一个矩阵每个格子的值都是x轴的值，第二个矩阵每个格子的值都是y轴的值\n",
    "    Z = himmelblau([X, Y])\n",
    "\n",
    "    fig = plt.figure('himmelblau')\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.plot_surface(X, Y, Z)\n",
    "    ax.view_init(60, -30)\n",
    "    ax.set_xlabel('x[0]')\n",
    "    ax.set_ylabel('y[1]')\n",
    "    plt.show()\n",
    "\n",
    "def gradient_decent():\n",
    "    x = torch.tensor([0., 0.], requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([x], lr=1e-3)\n",
    "\n",
    "    for step in range(20000):\n",
    "        pred = himmelblau(x)\n",
    "        optimizer.zero_grad() # 梯度清零，等价于net.zero_grad()\n",
    "        pred.backward() # 求解梯度\n",
    "        optimizer.step() # 更新参数，等价于w = w - lr*grad\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print('step {}: x={}, f(x)={}'.format(step, x.tolist(), pred.item()))\n",
    "\n",
    "plot_himmelblau(np.arange(-6, 6, 0.1), np.arange(-6, 6, 0.1))\n",
    "gradient_decent()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多分类问题"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类问题简述\n",
    "- 分类问题分为二分类问题和多分类问题。其中二分类问题在输出层使用sigmoid函数，多分类问题在输出层使用softmax函数。在多分类问题中，一般使用交叉熵损失函数。\n",
    "- 在分类问题中，一般使用交叉熵损失函数，其表达式为$\\text{loss}(y, f_{\\theta}(x)) = -\\sum_{i=1}^n y_i \\log f_{\\theta}(x_i)$，其中$y_i$为one-hot向量，$f_{\\theta}(x_i)$为softmax函数的输出。熵的物理意义是所含信息的多少，熵越大，所含信息越少。交叉熵损失函数的物理意义是KL-Divergence，即两个分布之间的差异。不使用MSE损失函数的原因是，MSE损失函数在分类问题中，其梯度会趋近于0，导致梯度消失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 验证entropy所代表的意义。熵越大，惊喜度越小，熵越小，惊喜度越大\n",
    "a = torch.tensor([0.25,0.25,0.25,0.25]); print('a', a)\n",
    "a_entropy = -torch.sum(a*torch.log2(a)); print('a的熵', a_entropy)\n",
    "b = torch.tensor([0.8,0.1,0.075,0.025]); print('b', b)\n",
    "b_entropy = -torch.sum(b*torch.log2(b)); print('b的熵', b_entropy)\n",
    "\n",
    "# cross entropy pytorch实现方式1, 需要先做softmax, 然后做交叉熵\n",
    "a = torch.randn(1,784); print('a', a)\n",
    "w = torch.randn(10,784); print('w', w)\n",
    "logits = a@w.t(); print('logits', logits) # 这里面叫logits没有什么特别的意义，就是定义的神经网络的一层输出结果。该输出一般会再接一个softmax layer输出normalize 后的概率，用于多分类\n",
    "pred = torch.softmax(logits, dim=1); print('pred', pred)\n",
    "print('after cross-entropy', -torch.log2(pred))\n",
    "\n",
    "# cross entropy pytorch实现方式2， 直接调用cross_entropy函数\n",
    "print(torch.nn.functional.cross_entropy(logits, torch.tensor([3]))) # 3表示第三个类别是正确的类别\n",
    "print(torch.nn.functional.nll_loss(pred.log(), torch.tensor([3]))) # 3表示第三个类别是正确的类别, nll_loss表示负对数似然损失函数，详情参考https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/NLLLoss_cn.html#cn-api-nn-loss-nllloss\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类问题绘图，one-hot以及加载数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类问题实战（基础版本）\n",
    "采用mnist数据集训练一个多分类器。\n",
    "- Dataloader: 用于加载数据集\n",
    "- Net: 用于定义网络结构\n",
    "- Train: 用于训练网络\n",
    "- Valid: 用于挑选网络\n",
    "- Test: 用于测试网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mnist import load_mnist_dataset\n",
    "\n",
    "mnist_train_loader,_,mnist_test_loader = load_mnist_dataset()\n",
    "\n",
    "\n",
    "# mnist 手写数字识别模型，基础实现\n",
    "# 定义模型(简单模型)\n",
    "class Net(torch.nn.Module):\n",
    "    w1, b1 = torch.randn(200, 784, requires_grad=True), torch.zeros(200, requires_grad=True)\n",
    "    w2, b2 = torch.randn(200, 200, requires_grad=True), torch.zeros(200, requires_grad=True)\n",
    "    w3, b3 = torch.randn(10, 200, requires_grad=True), torch.zeros(10, requires_grad=True)\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.w1)\n",
    "        torch.nn.init.kaiming_normal_(self.w2)\n",
    "        torch.nn.init.kaiming_normal_(self.w3)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.b1 + x@self.w1.t()\n",
    "        x = torch.relu(x)\n",
    "        x = self.b2 + x@self.w2.t()\n",
    "        x = torch.relu(x)\n",
    "        x = self.b3 + x@self.w3.t()\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "optimizer = torch.optim.SGD([net.w1, net.w2, net.w3, net.b1, net.b2, net.b3], lr=0.01, momentum=1e-3)\n",
    "criteria = torch.nn.CrossEntropyLoss() # 这里面criteria就是loss function，也就是损失函数，这里面使用的是交叉熵损失函数\n",
    "\n",
    "# 训练模型\n",
    "epochs = 10    \n",
    "for epoch in range(epochs):\n",
    "    for batch_index, (data, target) in enumerate(mnist_train_loader):\n",
    "        data = data.view(-1, 28*28) # reshape data, 将输入的28*28的图片reshape成一维向量\n",
    "        pred = net(data) # 前向传播\n",
    "        loss = criteria(pred, target)\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss.backward() # 计算损失函数\n",
    "        optimizer.step() # 更新参数\n",
    "        \n",
    "        if batch_index % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(epoch, batch_index * len(data), len(mnist_train_loader.dataset), 100. * batch_index / len(mnist_train_loader), loss.item()))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for test_batch_index, (test_data, test_target) in enumerate(mnist_test_loader):\n",
    "        test_data = test_data.view(-1, 28*28)\n",
    "        test_pred = net(test_data)\n",
    "        test_loss += criteria(test_pred, test_target).item()\n",
    "        pred = test_pred.argmax(dim=1)\n",
    "        correct += pred.eq(test_target).sum().item()\n",
    "        \n",
    "    test_loss /= len(mnist_test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(mnist_test_loader.dataset),\n",
    "        100. * correct / len(mnist_test_loader.dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类问题实战（进阶版本）\n",
    "- 使用GPU加速\n",
    "- 模型使用nn.Sequential\n",
    "- 激活函数使用LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mnist import load_mnist_dataset\n",
    "\n",
    "mnist_train_loader,_,mnist_test_loader = load_mnist_dataset()\n",
    "\n",
    "# mnist 手写数字识别模型，进阶版本\n",
    "# 定义模型\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 200),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 200),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 10),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "net = Net().to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=1e-3)\n",
    "criteria = torch.nn.CrossEntropyLoss().to(device) # 这里面criteria就是loss function，也就是损失函数，这里面使用的是交叉熵损失函数\n",
    "\n",
    "# 训练模型\n",
    "epochs = 10    \n",
    "for epoch in range(epochs):\n",
    "    for batch_index, (data, target) in enumerate(mnist_train_loader):\n",
    "        data = data.view(-1, 28*28).to(device) # reshape data, 将输入的28*28的图片reshape成一维向量\n",
    "        target = target.to(device)\n",
    "        pred = net(data) # 前向传播\n",
    "        loss = criteria(pred, target)\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss.backward() # 计算损失函数\n",
    "        optimizer.step() # 更新参数\n",
    "        \n",
    "        if batch_index % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(epoch, batch_index * len(data), len(mnist_train_loader.dataset), 100. * batch_index / len(mnist_train_loader), loss.item()))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for test_batch_index, (test_data, test_target) in enumerate(mnist_test_loader):\n",
    "        test_data = test_data.view(-1, 28*28).to(device)\n",
    "        test_target = test_target.to(device)\n",
    "        test_pred = net(test_data)\n",
    "        test_loss += criteria(test_pred, test_target).item()\n",
    "        pred = test_pred.argmax(dim=1)\n",
    "        correct += pred.eq(test_target).sum().item()\n",
    "        \n",
    "    test_loss /= len(mnist_test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(mnist_test_loader.dataset),\n",
    "        100. * correct / len(mnist_test_loader.dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化工具 Visdom\n",
    "- Visdom是一个网页可视化工具，可以用于可视化训练过程中的损失函数值，准确率等。\n",
    "- Q1: 如果在执行visdom的过程中，出现`downloading scripts, this may take a little while`的提示，可以修改`visdom/server.py`文件中的`self.download_scripts_and_run()`函数，将`download_scripts()`注释掉即可。\n",
    "- Q2: 对于visdom显示图像，需要将图像转成0-255才行，否则显示的全是黑色的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from mnist import load_mnist_dataset\n",
    "from visdom import Visdom\n",
    "\n",
    "mnist_train_loader,_,mnist_test_loader = load_mnist_dataset()\n",
    "\n",
    "\n",
    "# mnist 手写数字识别模型，进阶版本\n",
    "# 定义模型\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 200),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 200),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 10),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "net = Net().to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=1e-3)\n",
    "criteria = torch.nn.CrossEntropyLoss().to(device) # 这里面criteria就是loss function，也就是损失函数，这里面使用的是交叉熵损失函数\n",
    "\n",
    "# 增加Visdom\n",
    "viz = Visdom()\n",
    "viz.line([0.],[0.], win='train_loss', opts=dict(title='train loss')) # 绘制单条曲线\n",
    "viz.line([[0.0, 0.0]],[0.],win='test', opts=dict(title='test_loss&acc')) # 绘制多条曲线\n",
    "\n",
    "# 训练模型\n",
    "epochs = 10\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for batch_index, (data, target) in enumerate(mnist_train_loader):\n",
    "        \n",
    "\n",
    "        data = data.view(-1, 28*28).to(device) # reshape data, 将输入的28*28的图片reshape成一维向量\n",
    "        target = target.to(device)\n",
    "        pred = net(data) # 前向传播\n",
    "        loss = criteria(pred, target)\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss.backward() # 计算损失函数\n",
    "        optimizer.step() # 更新参数\n",
    "        global_step += batch_index\n",
    "        \n",
    "        if batch_index % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(epoch, batch_index * len(data), len(mnist_train_loader.dataset), 100. * batch_index / len(mnist_train_loader), loss.item()))\n",
    "            viz.line([loss.item()],[global_step], win='train_loss', update='append')\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for test_data, test_target in mnist_test_loader:\n",
    "        test_data = test_data.view(-1, 28*28).to(device)\n",
    "        test_target = test_target.to(device)\n",
    "        test_pred = net(test_data)\n",
    "        test_loss += criteria(test_pred, test_target).item()\n",
    "        pred = test_pred.argmax(dim=1)\n",
    "        correct += pred.eq(test_target).sum().item()\n",
    "        \n",
    "    test_loss /= len(mnist_test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(mnist_test_loader.dataset),\n",
    "        100. * correct / len(mnist_test_loader.dataset)))\n",
    "\n",
    "    viz.line([[test_loss, 100. * correct / len(mnist_test_loader.dataset)]],[epoch],win='test',update='append')\n",
    "    images = viz.images((test_data.view(-1,1,28,28)*0.3081+0.1307)*255,win='x',nrow=50,padding=4,opts=dict(title='images')) # 显示图像\n",
    "    viz.text(str(pred.detach().cpu().numpy()),win='pred',opts=dict(title='pred'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning中的一些Trick\n",
    "- Batch Normalization: 在每一层的输出上，都进行归一化，使得每一层的输出都是均值为0，方差为1的分布。其作用使得模型在训练阶段可以加快收敛速度。\n",
    "- Momentum: 在梯度下降的过程中，使用动量的方法。其作用是使得模型在训练过程中防止陷入局部最优解。\n",
    "- Batch Gradient Decent: 在梯度下降的过程中，使用batch gradient decent的方法。其作用是使得模型在训练过程中防止陷入局部最优解。\n",
    "- Adaptive Learning Rate: 在梯度下降的过程中，使用自适应学习率的方法。其作用是使得模型在训练过程中加快收敛速度。\n",
    "- Dropout: 在每一层的输出上，都进行dropout，即随机将一些神经元的输出置为0。其作用是防止模型过拟合。\n",
    "- Regularization: 在损失函数中，加入正则项。其作用是防止模型过拟合。\n",
    "- Early Stopping: 在训练过程中，如果验证集的损失函数值不再下降，则停止训练。其作用是防止模型过拟合。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何检测overfitting\n",
    "一般来说，在训练过程中，我们是拿不到test dataset的，test dataset的作用是评估最后模型的好坏。而在训练过程中，为了防止过拟合，需要将train dataset分成两部分，一部分用于训练，一部分用于验证。在训练过程中，我们可以使用validation dataset来评估模型的好坏，如果validation dataset的损失函数值不再下降，则停止训练。这样就可以防止模型过拟合。可以参考[PyTorch框架中使用Early Stopping](https://blog.csdn.net/t18438605018/article/details/123646329)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "基础版本,只是单独将train dataset划分为train dataset和validation dataset, 没有使用k-fold cross validation\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from mnist import load_mnist_dataset\n",
    "from visdom import Visdom\n",
    "from early_stopping import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(save_path='./')\n",
    "mnist_train_loader, mnist_validation_loader ,mnist_test_loader = load_mnist_dataset()\n",
    "\n",
    "# 定义模型\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 200),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 200),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 10),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "net = Net().to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=1e-3)\n",
    "criteria = torch.nn.CrossEntropyLoss().to(device) # 这里面criteria就是loss function，也就是损失函数，这里面使用的是交叉熵损失函数\n",
    "\n",
    "# 增加Visdom\n",
    "viz = Visdom()\n",
    "viz.line([0.],[0.], win='train_loss', opts=dict(title='train loss')) # 绘制单条曲线\n",
    "viz.line([[0.0, 0.0]],[0.],win='test', opts=dict(title='test_loss&acc')) # 绘制多条曲线\n",
    "\n",
    "# 训练模型\n",
    "epochs = 10\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for batch_index, (data, target) in enumerate(mnist_train_loader):\n",
    "        \n",
    "\n",
    "        data = data.view(-1, 28*28).to(device) # reshape data, 将输入的28*28的图片reshape成一维向量\n",
    "        target = target.to(device)\n",
    "        pred = net(data) # 前向传播\n",
    "        loss = criteria(pred, target)\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss.backward() # 计算损失函数\n",
    "        optimizer.step() # 更新参数\n",
    "        global_step += batch_index\n",
    "        \n",
    "        if batch_index % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(epoch, batch_index * len(data), len(mnist_train_loader.dataset), 100. * batch_index / len(mnist_train_loader), loss.item()))\n",
    "            viz.line([loss.item()],[global_step], win='train_loss', update='append')\n",
    "\n",
    "    validation_loss = 0\n",
    "    validation_correct = 0\n",
    "    for data, target in mnist_validation_loader:\n",
    "        data = data.view(-1, 28*28).to(device)\n",
    "        target = target.to(device)\n",
    "        pred = net(data)\n",
    "        validation_loss += criteria(pred, target).item()\n",
    "        pred = pred.argmax(dim=1)\n",
    "        validation_correct += pred.eq(target).sum().item()\n",
    "        \n",
    "    validation_loss /= len(mnist_validation_loader.dataset)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, validation_correct, len(mnist_validation_loader.dataset),\n",
    "        100. * validation_correct / len(mnist_validation_loader.dataset)))\n",
    "\n",
    "    viz.line([[validation_loss, 100. * validation_correct / len(mnist_validation_loader.dataset)]],[epoch],win='validation',update='append')\n",
    "    images = viz.images((data.view(-1,1,28,28)*0.3081+0.1307)*255,win='x',nrow=50,padding=4,opts=dict(title='images')) # 显示图像\n",
    "    viz.text(str(pred.detach().cpu().numpy()),win='pred',opts=dict(title='pred'))\n",
    "\n",
    "    # early stopping judge\n",
    "    early_stopping(validation_loss, net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "高阶版本,使用k-fold cross validation\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from mnist import load_mnist_dataset\n",
    "from visdom import Visdom\n",
    "from early_stopping import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(save_path='./')\n",
    "mnist_train_loader, mnist_validation_loader ,mnist_test_loader = load_mnist_dataset()\n",
    "\n",
    "# 定义模型\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 200),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 200),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 10),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "net = Net().to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=1e-3)\n",
    "criteria = torch.nn.CrossEntropyLoss().to(device) # 这里面criteria就是loss function，也就是损失函数，这里面使用的是交叉熵损失函数\n",
    "\n",
    "# 增加Visdom\n",
    "viz = Visdom()\n",
    "viz.line([0.],[0.], win='train_loss', opts=dict(title='train loss')) # 绘制单条曲线\n",
    "viz.line([[0.0, 0.0]],[0.],win='test', opts=dict(title='test_loss&acc')) # 绘制多条曲线\n",
    "\n",
    "# 训练模型\n",
    "epochs = 10\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for batch_index, (data, target) in enumerate(mnist_train_loader):\n",
    "        \n",
    "\n",
    "        data = data.view(-1, 28*28).to(device) # reshape data, 将输入的28*28的图片reshape成一维向量\n",
    "        target = target.to(device)\n",
    "        pred = net(data) # 前向传播\n",
    "        loss = criteria(pred, target)\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss.backward() # 计算损失函数\n",
    "        optimizer.step() # 更新参数\n",
    "        global_step += batch_index\n",
    "        \n",
    "        if batch_index % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(epoch, batch_index * len(data), len(mnist_train_loader.dataset), 100. * batch_index / len(mnist_train_loader), loss.item()))\n",
    "            viz.line([loss.item()],[global_step], win='train_loss', update='append')\n",
    "\n",
    "    validation_loss = 0\n",
    "    validation_correct = 0\n",
    "    for data, target in mnist_validation_loader:\n",
    "        data = data.view(-1, 28*28).to(device)\n",
    "        target = target.to(device)\n",
    "        pred = net(data)\n",
    "        validation_loss += criteria(pred, target).item()\n",
    "        pred = pred.argmax(dim=1)\n",
    "        validation_correct += pred.eq(target).sum().item()\n",
    "        \n",
    "    validation_loss /= len(mnist_validation_loader.dataset)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, validation_correct, len(mnist_validation_loader.dataset),\n",
    "        100. * validation_correct / len(mnist_validation_loader.dataset)))\n",
    "\n",
    "    viz.line([[validation_loss, 100. * validation_correct / len(mnist_validation_loader.dataset)]],[epoch],win='validation',update='append')\n",
    "    images = viz.images((data.view(-1,1,28,28)*0.3081+0.1307)*255,win='x',nrow=50,padding=4,opts=dict(title='images')) # 显示图像\n",
    "    viz.text(str(pred.detach().cpu().numpy()),win='pred',opts=dict(title='pred'))\n",
    "\n",
    "    # early stopping judge\n",
    "    early_stopping(validation_loss, net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何减轻overfitting\n",
    "- Regularization: 在损失函数中，加入正则项。其作用是防止模型过拟合。在pytorch中，L2-regularization通过在optimizer中设置weight_decay参数来实现, L1-regularization通过自建一个loss函数来实现utils.l1_penalty()。\n",
    "- Dropout: 在每一层的输出上，都进行dropout，即随机将一些神经元的输出置为0。其作用是防止模型过拟合。在pytorch中，dropout通过在网络中设置nn.Dropout()来实现。\n",
    "- Early Stopping: 提前选定一个epoch，如果在这个epoch之后，验证集的损失函数值不再下降，则停止训练。其作用是防止模型过拟合。在pytorch中，early stopping通过自建一个EarlyStopping类来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mnist import load_mnist_dataset\n",
    "from visdom import Visdom\n",
    "from early_stopping import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(save_path='./')\n",
    "mnist_train_loader, mnist_validation_loader ,mnist_test_loader = load_mnist_dataset()\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    dropout = 0.5\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 200),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 200),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 10),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "net = Net().to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=1e-3,weight_decay=0.01)\n",
    "criteria = torch.nn.CrossEntropyLoss().to(device) # 这里面criteria就是loss function，也就是损失函数，这里面使用的是交叉熵损失函数\n",
    "\n",
    "# 增加Visdom\n",
    "viz = Visdom()\n",
    "viz.line([0.],[0.], win='train_loss', opts=dict(title='train loss')) # 绘制单条曲线\n",
    "viz.line([[0.0, 0.0]],[0.],win='test', opts=dict(title='test_loss&acc')) # 绘制多条曲线\n",
    "\n",
    "# 训练模型\n",
    "epochs = 10\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for batch_index, (data, target) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        net.train() # 训练模式\n",
    "        data = data.view(-1, 28*28).to(device) # reshape data, 将输入的28*28的图片reshape成一维向量\n",
    "        target = target.to(device)\n",
    "        pred = net(data) # 前向传播\n",
    "        loss = criteria(pred, target)\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss.backward() # 计算损失函数\n",
    "        optimizer.step() # 更新参数\n",
    "        global_step += batch_index\n",
    "        \n",
    "        if batch_index % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(epoch, batch_index * len(data), len(mnist_train_loader.dataset), 100. * batch_index / len(mnist_train_loader), loss.item()))\n",
    "            viz.line([loss.item()],[global_step], win='train_loss', update='append')\n",
    "\n",
    "    validation_loss = 0\n",
    "    validation_correct = 0\n",
    "    net.eval() # 训练模式\n",
    "    for data, target in mnist_validation_loader:\n",
    "        data = data.view(-1, 28*28).to(device)\n",
    "        target = target.to(device)\n",
    "        pred = net(data)\n",
    "        validation_loss += criteria(pred, target).item()\n",
    "        pred = pred.argmax(dim=1)\n",
    "        validation_correct += pred.eq(target).sum().item()\n",
    "        \n",
    "    validation_loss /= len(mnist_validation_loader.dataset)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, validation_correct, len(mnist_validation_loader.dataset),\n",
    "        100. * validation_correct / len(mnist_validation_loader.dataset)))\n",
    "\n",
    "    viz.line([[validation_loss, 100. * validation_correct / len(mnist_validation_loader.dataset)]],[epoch],win='validation',update='append')\n",
    "    images = viz.images((data.view(-1,1,28,28)*0.3081+0.1307)*255,win='x',nrow=50,padding=4,opts=dict(title='images')) # 显示图像\n",
    "    viz.text(str(pred.detach().cpu().numpy()),win='pred',opts=dict(title='pred'))\n",
    "\n",
    "    # early stopping judge\n",
    "    early_stopping(validation_loss, net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum, Learning Rate Decay等Trick\n",
    "- Momentum: 其具有实际的物理意义，类似于从高处向下滚动的小球，可以冲出局部最优解。在pytorch中，momentum通过在optimizer中设置momentum参数来实现。\n",
    "- Learning Rate Decay: 其直观想法是随着训练的进行，越来越接近最优解，所以学习率应该越来越小。在pytorch中，learning rate decay通过在optimizer中设置不同的lr_scheduler来实现。不同的lr_scheduler参考[lr_scheduler介绍](https://blog.csdn.net/weiman1/article/details/125647517)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mnist import load_mnist_dataset\n",
    "from visdom import Visdom\n",
    "from early_stopping import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(save_path='./')\n",
    "mnist_train_loader, mnist_validation_loader ,mnist_test_loader = load_mnist_dataset()\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    dropout = 0.5\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 200),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 200),\n",
    "            torch.nn.Dropout(self.dropout),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Linear(200, 10),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "net = Net().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=1e-3,weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min')\n",
    "\n",
    "criteria = torch.nn.CrossEntropyLoss().to(device) # 这里面criteria就是loss function，也就是损失函数，这里面使用的是交叉熵损失函数\n",
    "\n",
    "# 增加Visdom\n",
    "viz = Visdom()\n",
    "viz.line([0.],[0.], win='train_loss', opts=dict(title='train loss')) # 绘制单条曲线\n",
    "viz.line([[0.0, 0.0]],[0.],win='test', opts=dict(title='test_loss&acc')) # 绘制多条曲线\n",
    "\n",
    "# 训练模型\n",
    "epochs = 10\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    for batch_index, (data, target) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        net.train() # 训练模式\n",
    "        data = data.view(-1, 28*28).to(device) # reshape data, 将输入的28*28的图片reshape成一维向量\n",
    "        target = target.to(device)\n",
    "        pred = net(data) # 前向传播\n",
    "        loss = criteria(pred, target)\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss.backward() # 计算损失函数\n",
    "        optimizer.step() # 更新参数\n",
    "        global_step += batch_index\n",
    "\n",
    "        scheduler.step(loss) # 更新学习率\n",
    "        \n",
    "        if batch_index % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(epoch, batch_index * len(data), len(mnist_train_loader.dataset), 100. * batch_index / len(mnist_train_loader), loss.item()))\n",
    "            viz.line([loss.item()],[global_step], win='train_loss', update='append')\n",
    "\n",
    "    validation_loss = 0\n",
    "    validation_correct = 0\n",
    "    net.eval() # 训练模式\n",
    "    for data, target in mnist_validation_loader:\n",
    "        data = data.view(-1, 28*28).to(device)\n",
    "        target = target.to(device)\n",
    "        pred = net(data)\n",
    "        validation_loss += criteria(pred, target).item()\n",
    "        pred = pred.argmax(dim=1)\n",
    "        validation_correct += pred.eq(target).sum().item()\n",
    "        \n",
    "    validation_loss /= len(mnist_validation_loader.dataset)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, validation_correct, len(mnist_validation_loader.dataset),\n",
    "        100. * validation_correct / len(mnist_validation_loader.dataset)))\n",
    "\n",
    "    viz.line([[validation_loss, 100. * validation_correct / len(mnist_validation_loader.dataset)]],[epoch],win='validation',update='append')\n",
    "    images = viz.images((data.view(-1,1,28,28)*0.3081+0.1307)*255,win='x',nrow=50,padding=4,opts=dict(title='images')) # 显示图像\n",
    "    viz.text(str(pred.detach().cpu().numpy()),win='pred',opts=dict(title='pred'))\n",
    "\n",
    "    # early stopping judge\n",
    "    early_stopping(validation_loss, net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_tw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
